{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489953ec",
   "metadata": {},
   "source": [
    "# Feature Engineering for Synthetic Finance Anomaly Detection\n",
    "\n",
    "This notebook demonstrates the process of building an enriched transaction dataset for anomaly detection in personal finance. It merges multiple synthetic financial tables into a single transaction-level DataFrame and computes features that are useful for detecting suspicious or unusual transactions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The feature engineering process involves:\n",
    "1. **Data Loading**: Loading cleaned datasets from the processed data directory\n",
    "2. **Table Merging**: Combining multiple financial tables to create enriched transactions\n",
    "3. **Feature Creation**: Computing derived features for anomaly detection\n",
    "4. **Data Validation**: Ensuring data quality and consistency\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The synthetic dataset contains the following tables:\n",
    "- **transactions** (48,042 rows) - Core transaction data\n",
    "- **transaction_types** - Transaction type classifications\n",
    "- **accounts** - Account information with balances\n",
    "- **account_types** - Account type classifications (Checking, Savings, etc.)\n",
    "- **account_statuses** - Account status (Active, Inactive, Closed)\n",
    "- **customers** - Customer demographic information\n",
    "- **customer_types** - Customer type classifications\n",
    "- **addresses** - Customer address information\n",
    "- **branches** - Bank branch information\n",
    "- **loans** - Loan information linked to accounts\n",
    "- **loan_statuses** - Loan status classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa345b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded accounts_cleaned: 1635 rows, 6 columns\n",
      "Loaded account_statuses: 3 rows, 2 columns\n",
      "Loaded account_types: 5 rows, 2 columns\n",
      "Loaded addresses_cleaned: 1210 rows, 4 columns\n",
      "Loaded branches_cleaned: 50 rows, 3 columns\n",
      "Loaded customers_cleaned: 1058 rows, 6 columns\n",
      "Loaded customer_types: 3 rows, 2 columns\n",
      "Loaded loans_cleaned: 316 rows, 7 columns\n",
      "Loaded loan_statuses: 3 rows, 2 columns\n",
      "Loaded transactions_cleaned: 48042 rows, 8 columns\n",
      "Loaded transaction_types: 4 rows, 2 columns\n",
      "\n",
      "Total DataFrames created: 11\n",
      "\n",
      "Available DataFrames: ['accounts_cleaned', 'account_statuses', 'account_types', 'addresses_cleaned', 'branches_cleaned', 'customers_cleaned', 'customer_types', 'loans_cleaned', 'loan_statuses', 'transactions_cleaned', 'transaction_types']\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Get all CSV files from the processed data directory\n",
    "raw_data_path = \"../data/processed\"\n",
    "csv_files = glob.glob(os.path.join(raw_data_path, \"*.csv\"))\n",
    "\n",
    "# Create a dictionary of DataFrames for easy access\n",
    "dataframes = []\n",
    "file_names = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # Extract filename without extension for reference\n",
    "    file_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "    \n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Store DataFrame and filename\n",
    "    dataframes.append(df)\n",
    "    file_names.append(file_name)\n",
    "    \n",
    "    print(f\"Loaded {file_name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "print(f\"\\nTotal DataFrames created: {len(dataframes)}\")\n",
    "\n",
    "# Create dictionary for easy access\n",
    "df_dict = dict(zip(file_names, dataframes))\n",
    "print(f\"\\nAvailable DataFrames: {list(df_dict.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b944d8",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "The first step involves loading all the cleaned datasets from the processed data directory. This includes:\n",
    "\n",
    "- **Transaction data**: Core transaction records with amounts, dates, and account references\n",
    "- **Reference tables**: Lookup tables for transaction types, account types, customer types, etc.\n",
    "- **Account data**: Account balances, types, and status information\n",
    "- **Customer data**: Customer demographics and contact information\n",
    "- **Loan data**: Loan information associated with accounts\n",
    "- **Branch data**: Bank branch information\n",
    "\n",
    "All datasets are loaded into a dictionary for easy access during the merging process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26381201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames assigned successfully!\n",
      "Transaction data shape: (48042, 8)\n",
      "Account data shape: (1635, 6)\n",
      "Customer data shape: (1058, 6)\n"
     ]
    }
   ],
   "source": [
    "# Assign datasets to individual variables for clarity\n",
    "df_transactions = df_dict['transactions_cleaned']\n",
    "df_transaction_types = df_dict['transaction_types']\n",
    "df_account_statuses = df_dict['account_statuses']\n",
    "df_account_types = df_dict['account_types']\n",
    "df_accounts = df_dict['accounts_cleaned']\n",
    "df_branches = df_dict['branches_cleaned']\n",
    "df_customer_types = df_dict['customer_types']\n",
    "df_customers = df_dict['customers_cleaned']\n",
    "df_loan_statuses = df_dict['loan_statuses']\n",
    "df_loans = df_dict['loans_cleaned']\n",
    "df_addresses = df_dict['addresses_cleaned']\n",
    "\n",
    "print(\"DataFrames assigned successfully!\")\n",
    "print(f\"Transaction data shape: {df_transactions.shape}\")\n",
    "print(f\"Account data shape: {df_accounts.shape}\")\n",
    "print(f\"Customer data shape: {df_customers.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e234705",
   "metadata": {},
   "source": [
    "## Step 2: Table Merging Strategy\n",
    "\n",
    "The goal is to create a single enriched transaction DataFrame containing all relevant information about:\n",
    "- **Origin accounts**: Source account details, customer info, and loan data\n",
    "- **Destination accounts**: Target account details, customer info, and loan data\n",
    "- **Transaction context**: Transaction type, branch information, and timing\n",
    "\n",
    "### Merging Process:\n",
    "\n",
    "1. **Transaction Types**: Add transaction type names to transactions\n",
    "2. **Origin Account Details**: Merge account, account type, and account status for origin accounts\n",
    "3. **Destination Account Details**: Merge account, account type, and account status for destination accounts\n",
    "4. **Customer Information**: Add customer demographics for both origin and destination\n",
    "5. **Loan Information**: Aggregate loan data for both origin and destination accounts\n",
    "6. **Branch Information**: Add branch details for transaction context\n",
    "\n",
    "This creates a comprehensive dataset where each row represents a transaction with full context about all parties involved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27e36450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table merging function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def build_enriched_transactions(transactions, transaction_types,\n",
    "                                accounts, account_types, account_statuses,\n",
    "                                customers, customer_types, addresses,\n",
    "                                branches, loans, loan_statuses):\n",
    "    \"\"\"\n",
    "    Build an enriched transaction dataset by merging multiple financial tables.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    transactions : pd.DataFrame\n",
    "        Core transaction data\n",
    "    transaction_types : pd.DataFrame\n",
    "        Transaction type lookup table\n",
    "    accounts : pd.DataFrame\n",
    "        Account information\n",
    "    account_types : pd.DataFrame\n",
    "        Account type lookup table\n",
    "    account_statuses : pd.DataFrame\n",
    "        Account status lookup table\n",
    "    customers : pd.DataFrame\n",
    "        Customer information\n",
    "    customer_types : pd.DataFrame\n",
    "        Customer type lookup table\n",
    "    addresses : pd.DataFrame\n",
    "        Address information\n",
    "    branches : pd.DataFrame\n",
    "        Branch information\n",
    "    loans : pd.DataFrame\n",
    "        Loan information\n",
    "    loan_statuses : pd.DataFrame\n",
    "        Loan status lookup table\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Enriched transaction dataset with all relevant information\n",
    "    \"\"\"\n",
    "    \n",
    "    # === STEP 1: Transaction types ===\n",
    "    tx = transactions.merge(\n",
    "        transaction_types,\n",
    "        on=\"TransactionTypeID\",\n",
    "        how=\"left\"\n",
    "    ).rename(columns={\"TypeName\": \"TransactionTypeName\"})\n",
    "\n",
    "    # === STEP 2: Origin account details ===\n",
    "    tx = tx.merge(\n",
    "        accounts.add_prefix(\"Origin_\"),\n",
    "        left_on=\"AccountOriginID\",\n",
    "        right_on=\"Origin_AccountID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    tx = tx.merge(\n",
    "        account_types.add_prefix(\"Origin_\"),\n",
    "        left_on=\"Origin_AccountTypeID\",\n",
    "        right_on=\"Origin_AccountTypeID\",\n",
    "        how=\"left\"\n",
    "    ).rename(columns={\"Origin_TypeName\": \"Origin_AccountType\"})\n",
    "    tx = tx.merge(\n",
    "        account_statuses.add_prefix(\"Origin_\"),\n",
    "        left_on=\"Origin_AccountStatusID\",\n",
    "        right_on=\"Origin_AccountStatusID\",\n",
    "        how=\"left\"\n",
    "    ).rename(columns={\"Origin_StatusName\": \"Origin_AccountStatus\"})\n",
    "\n",
    "    # === STEP 3: Destination account details ===\n",
    "    tx = tx.merge(\n",
    "        accounts.add_prefix(\"Dest_\"),\n",
    "        left_on=\"AccountDestinationID\",\n",
    "        right_on=\"Dest_AccountID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    tx = tx.merge(\n",
    "        account_types.add_prefix(\"Dest_\"),\n",
    "        left_on=\"Dest_AccountTypeID\",\n",
    "        right_on=\"Dest_AccountTypeID\",\n",
    "        how=\"left\"\n",
    "    ).rename(columns={\"Dest_TypeName\": \"Dest_AccountType\"})\n",
    "    tx = tx.merge(\n",
    "        account_statuses.add_prefix(\"Dest_\"),\n",
    "        left_on=\"Dest_AccountStatusID\",\n",
    "        right_on=\"Dest_AccountStatusID\",\n",
    "        how=\"left\"\n",
    "    ).rename(columns={\"Dest_StatusName\": \"Dest_AccountStatus\"})\n",
    "\n",
    "    # === STEP 4: Customer info (origin & dest) ===\n",
    "    customers_full = customers.merge(\n",
    "        customer_types,\n",
    "        on=\"CustomerTypeID\",\n",
    "        how=\"left\"\n",
    "    ).merge(\n",
    "        addresses,\n",
    "        on=\"AddressID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Origin customer\n",
    "    tx = tx.merge(\n",
    "        customers_full.add_prefix(\"Origin_\"),\n",
    "        left_on=\"Origin_CustomerID\",\n",
    "        right_on=\"Origin_CustomerID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Destination customer\n",
    "    tx = tx.merge(\n",
    "        customers_full.add_prefix(\"Dest_\"),\n",
    "        left_on=\"Dest_CustomerID\",\n",
    "        right_on=\"Dest_CustomerID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # === STEP 5: Branch info ===\n",
    "    branches_full = branches.merge(\n",
    "        addresses.add_prefix(\"Branch_\"),\n",
    "        left_on=\"AddressID\",\n",
    "        right_on=\"Branch_AddressID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    tx = tx.merge(\n",
    "        branches_full.add_prefix(\"Branch_\"),\n",
    "        left_on=\"BranchID\",\n",
    "        right_on=\"Branch_BranchID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # === STEP 6: Loan features ===\n",
    "    # Aggregate loan metrics per account\n",
    "    loan_features = loans.groupby(\"AccountID\").agg(\n",
    "        LoanCount=(\"LoanID\", \"count\"),\n",
    "        TotalPrincipal=(\"PrincipalAmount\", \"sum\"),\n",
    "        AvgInterestRate=(\"InterestRate\", \"mean\"),\n",
    "        MaxInterestRate=(\"InterestRate\", \"max\"),\n",
    "        MinInterestRate=(\"InterestRate\", \"min\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # Loan status counts per account\n",
    "    loan_status_pivot = loans.merge(\n",
    "        loan_statuses,\n",
    "        on=\"LoanStatusID\",\n",
    "        how=\"left\"\n",
    "    ).groupby([\"AccountID\", \"StatusName\"]).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    # Merge loan features for origin accounts\n",
    "    tx = tx.merge(\n",
    "        loan_features.add_prefix(\"Origin_\"),\n",
    "        left_on=\"Origin_AccountID\",\n",
    "        right_on=\"Origin_AccountID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge loan features for destination accounts\n",
    "    tx = tx.merge(\n",
    "        loan_features.add_prefix(\"Dest_\"),\n",
    "        left_on=\"Dest_AccountID\",\n",
    "        right_on=\"Dest_AccountID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge loan status counts for origin accounts\n",
    "    tx = tx.merge(\n",
    "        loan_status_pivot.add_prefix(\"Origin_LoanStatus_\"),\n",
    "        left_on=\"Origin_AccountID\",\n",
    "        right_on=\"Origin_LoanStatus_AccountID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge loan status counts for destination accounts\n",
    "    tx = tx.merge(\n",
    "        loan_status_pivot.add_prefix(\"Dest_LoanStatus_\"),\n",
    "        left_on=\"Dest_AccountID\",\n",
    "        right_on=\"Dest_LoanStatus_AccountID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return tx\n",
    "\n",
    "print(\"Table merging function defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa845a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting table merging process...\n",
      "âœ… Merging completed!\n",
      "Final dataset shape: (48042, 68)\n",
      "Number of columns: 68\n",
      "Number of rows: 48042\n",
      "\n",
      "ğŸ“Š Dataset Overview:\n",
      "Memory usage: 96.47 MB\n",
      "Missing values: 764446\n",
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Execute the table merging process\n",
    "print(\"Starting table merging process...\")\n",
    "\n",
    "df_final = build_enriched_transactions(\n",
    "    df_transactions, df_transaction_types,\n",
    "    df_accounts, df_account_types, df_account_statuses,\n",
    "    df_customers, df_customer_types, df_addresses,\n",
    "    df_branches, df_loans, df_loan_statuses\n",
    ")\n",
    "\n",
    "print(f\"âœ… Merging completed!\")\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"Number of columns: {df_final.shape[1]}\")\n",
    "print(f\"Number of rows: {df_final.shape[0]}\")\n",
    "\n",
    "# Display basic information about the merged dataset\n",
    "print(\"\\nğŸ“Š Dataset Overview:\")\n",
    "print(f\"Memory usage: {df_final.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Missing values: {df_final.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df_final.duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32452486",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "\n",
    "After merging all tables, we create derived features that are useful for anomaly detection. These features fall into several categories:\n",
    "\n",
    "### Feature Categories:\n",
    "\n",
    "1. **Amount-based Features**: Ratios and relationships between transaction amounts and account balances\n",
    "2. **Account Status Features**: Flags for inactive or problematic accounts\n",
    "3. **Customer Demographics**: Age calculations and customer type information\n",
    "4. **Loan-related Features**: Leverage ratios and loan status indicators\n",
    "5. **Temporal Features**: Time-based patterns in transactions\n",
    "6. **Anomaly Flags**: Heuristic flags for potentially suspicious transactions\n",
    "\n",
    "### Feature Engineering Goals:\n",
    "\n",
    "- **Detect unusual transaction patterns**: Large transfers, unusual timing, etc.\n",
    "- **Identify risky accounts**: Inactive accounts, high leverage, etc.\n",
    "- **Capture customer behavior**: Age-related patterns, customer type differences\n",
    "- **Enable machine learning**: Create features suitable for anomaly detection algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f73e7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def compute_transaction_features(df):\n",
    "    \"\"\"\n",
    "    Compute derived features for anomaly detection from the enriched transaction dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Enriched transaction dataset with merged account, customer, and loan information\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataset with additional engineered features\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"ğŸ”§ Computing transaction features...\")\n",
    "    \n",
    "    # === AMOUNT-BASED FEATURES ===\n",
    "    print(\"  â†’ Amount-based features\")\n",
    "    \n",
    "    # Amount ratios (avoid division by zero)\n",
    "    df['Amount_to_OriginBalance'] = df['Amount'] / df['Origin_Balance'].replace(0, np.nan)\n",
    "    df['Amount_to_DestBalance'] = df['Amount'] / df['Dest_Balance'].replace(0, np.nan)\n",
    "    \n",
    "    # Amount relative to average transaction size\n",
    "    avg_transaction = df['Amount'].mean()\n",
    "    df['Amount_to_AvgTransaction'] = df['Amount'] / avg_transaction\n",
    "    \n",
    "    # === ACCOUNT STATUS FEATURES ===\n",
    "    print(\"  â†’ Account status features\")\n",
    "    \n",
    "    # Account inactivity flags\n",
    "    df['Origin_AccountInactive'] = df['Origin_AccountStatus'].isin(['Inactive', 'Closed']).astype(int)\n",
    "    df['Dest_AccountInactive'] = df['Dest_AccountStatus'].isin(['Inactive', 'Closed']).astype(int)\n",
    "    \n",
    "    # Account type flags\n",
    "    df['Origin_IsChecking'] = (df['Origin_AccountType'] == 'Checking').astype(int)\n",
    "    df['Dest_IsChecking'] = (df['Dest_AccountType'] == 'Checking').astype(int)\n",
    "    df['Origin_IsSavings'] = (df['Origin_AccountType'] == 'Savings').astype(int)\n",
    "    df['Dest_IsSavings'] = (df['Dest_AccountType'] == 'Savings').astype(int)\n",
    "    \n",
    "    # === CUSTOMER DEMOGRAPHIC FEATURES ===\n",
    "    print(\"  â†’ Customer demographic features\")\n",
    "    \n",
    "    # Calculate customer ages\n",
    "    today = pd.Timestamp.today()\n",
    "    df['Origin_Age'] = (today - pd.to_datetime(df['Origin_DateOfBirth'], errors='coerce')).dt.days // 365\n",
    "    df['Dest_Age'] = (today - pd.to_datetime(df['Dest_DateOfBirth'], errors='coerce')).dt.days // 365\n",
    "    \n",
    "    # Age difference between origin and destination customers\n",
    "    df['Age_Difference'] = df['Origin_Age'] - df['Dest_Age']\n",
    "    \n",
    "    # Customer type flags\n",
    "    df['Origin_IsIndividual'] = (df['Origin_TypeName'] == 'Individual').astype(int)\n",
    "    df['Dest_IsIndividual'] = (df['Dest_TypeName'] == 'Individual').astype(int)\n",
    "    df['Origin_IsBusiness'] = (df['Origin_TypeName'] == 'Small Business').astype(int)\n",
    "    df['Dest_IsBusiness'] = (df['Dest_TypeName'] == 'Small Business').astype(int)\n",
    "    \n",
    "    # === LOAN-RELATED FEATURES ===\n",
    "    print(\"  â†’ Loan-related features\")\n",
    "    \n",
    "    # Loan leverage ratios (debt to balance)\n",
    "    df['Origin_LoanLeverage'] = df['Origin_TotalPrincipal'] / df['Origin_Balance'].replace(0, np.nan)\n",
    "    df['Dest_LoanLeverage'] = df['Dest_TotalPrincipal'] / df['Dest_Balance'].replace(0, np.nan)\n",
    "    \n",
    "    # Loan count flags\n",
    "    df['Origin_HasLoans'] = (df['Origin_LoanCount'] > 0).astype(int)\n",
    "    df['Dest_HasLoans'] = (df['Dest_LoanCount'] > 0).astype(int)\n",
    "    \n",
    "    # High interest rate flags (above 10%)\n",
    "    df['Origin_HighInterest'] = (df['Origin_AvgInterestRate'] > 10).astype(int)\n",
    "    df['Dest_HighInterest'] = (df['Dest_AvgInterestRate'] > 10).astype(int)\n",
    "    \n",
    "    # === TEMPORAL FEATURES ===\n",
    "    print(\"  â†’ Temporal features\")\n",
    "    \n",
    "    # Extract time components\n",
    "    df['TransactionHour'] = pd.to_datetime(df['TransactionDate']).dt.hour\n",
    "    df['TransactionWeekday'] = pd.to_datetime(df['TransactionDate']).dt.dayofweek\n",
    "    df['TransactionMonth'] = pd.to_datetime(df['TransactionDate']).dt.month\n",
    "    df['TransactionQuarter'] = pd.to_datetime(df['TransactionDate']).dt.quarter\n",
    "    \n",
    "    # Time-based flags\n",
    "    df['IsWeekend'] = (df['TransactionWeekday'] >= 5).astype(int)\n",
    "    df['IsBusinessHours'] = ((df['TransactionHour'] >= 9) & (df['TransactionHour'] <= 17)).astype(int)\n",
    "    df['IsNightTime'] = ((df['TransactionHour'] >= 22) | (df['TransactionHour'] <= 6)).astype(int)\n",
    "    \n",
    "    # === ANOMALY FLAGS ===\n",
    "    print(\"  â†’ Anomaly detection flags\")\n",
    "    \n",
    "    # Large transfer flags\n",
    "    df['LargeTransferFlag'] = (df['Amount_to_OriginBalance'] > 0.5).astype(int)\n",
    "    df['VeryLargeTransferFlag'] = (df['Amount_to_OriginBalance'] > 0.9).astype(int)\n",
    "    \n",
    "    # Unusual timing flags\n",
    "    df['UnusualTimingFlag'] = ((df['IsNightTime'] == 1) | (df['IsWeekend'] == 1)).astype(int)\n",
    "    \n",
    "    # High-risk account combinations\n",
    "    df['HighRiskFlag'] = ((df['Origin_AccountInactive'] == 1) | \n",
    "                         (df['Dest_AccountInactive'] == 1) |\n",
    "                         (df['Origin_LoanLeverage'] > 2) |\n",
    "                         (df['Dest_LoanLeverage'] > 2)).astype(int)\n",
    "    \n",
    "    # Cross-customer type transfers (business to individual, etc.)\n",
    "    df['CrossTypeTransfer'] = (df['Origin_TypeName'] != df['Dest_TypeName']).astype(int)\n",
    "    \n",
    "    print(\"âœ… Feature engineering completed!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Feature engineering function defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c3eb710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48042 entries, 0 to 48041\n",
      "Data columns (total 68 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   TransactionID                48042 non-null  int64  \n",
      " 1   AccountOriginID              48042 non-null  int64  \n",
      " 2   AccountDestinationID         48042 non-null  int64  \n",
      " 3   TransactionTypeID            48042 non-null  int64  \n",
      " 4   Amount                       48042 non-null  float64\n",
      " 5   TransactionDate              48042 non-null  object \n",
      " 6   BranchID                     48042 non-null  int64  \n",
      " 7   Description                  48042 non-null  object \n",
      " 8   TransactionTypeName          48042 non-null  object \n",
      " 9   Origin_AccountID             47581 non-null  float64\n",
      " 10  Origin_CustomerID            47581 non-null  float64\n",
      " 11  Origin_AccountTypeID         47581 non-null  float64\n",
      " 12  Origin_AccountStatusID       47581 non-null  float64\n",
      " 13  Origin_Balance               47581 non-null  float64\n",
      " 14  Origin_OpeningDate           47581 non-null  object \n",
      " 15  Origin_AccountType           47581 non-null  object \n",
      " 16  Origin_AccountStatus         47581 non-null  object \n",
      " 17  Dest_AccountID               47562 non-null  float64\n",
      " 18  Dest_CustomerID              47562 non-null  float64\n",
      " 19  Dest_AccountTypeID           47562 non-null  float64\n",
      " 20  Dest_AccountStatusID         47562 non-null  float64\n",
      " 21  Dest_Balance                 47562 non-null  float64\n",
      " 22  Dest_OpeningDate             47562 non-null  object \n",
      " 23  Dest_AccountType             47562 non-null  object \n",
      " 24  Dest_AccountStatus           47562 non-null  object \n",
      " 25  Origin_FirstName             45541 non-null  object \n",
      " 26  Origin_LastName              45541 non-null  object \n",
      " 27  Origin_DateOfBirth           45541 non-null  object \n",
      " 28  Origin_AddressID             45541 non-null  float64\n",
      " 29  Origin_CustomerTypeID        45541 non-null  float64\n",
      " 30  Origin_TypeName              45541 non-null  object \n",
      " 31  Origin_Street                45541 non-null  object \n",
      " 32  Origin_City                  45541 non-null  object \n",
      " 33  Origin_Country               45541 non-null  object \n",
      " 34  Dest_FirstName               45415 non-null  object \n",
      " 35  Dest_LastName                45415 non-null  object \n",
      " 36  Dest_DateOfBirth             45415 non-null  object \n",
      " 37  Dest_AddressID               45415 non-null  float64\n",
      " 38  Dest_CustomerTypeID          45415 non-null  float64\n",
      " 39  Dest_TypeName                45415 non-null  object \n",
      " 40  Dest_Street                  45415 non-null  object \n",
      " 41  Dest_City                    45415 non-null  object \n",
      " 42  Dest_Country                 45415 non-null  object \n",
      " 43  Branch_BranchID              48042 non-null  int64  \n",
      " 44  Branch_BranchName            48042 non-null  object \n",
      " 45  Branch_AddressID             48042 non-null  int64  \n",
      " 46  Branch_Branch_AddressID      48042 non-null  int64  \n",
      " 47  Branch_Branch_Street         48042 non-null  object \n",
      " 48  Branch_Branch_City           48042 non-null  object \n",
      " 49  Branch_Branch_Country        48042 non-null  object \n",
      " 50  Origin_LoanCount             8621 non-null   float64\n",
      " 51  Origin_TotalPrincipal        8621 non-null   float64\n",
      " 52  Origin_AvgInterestRate       8621 non-null   float64\n",
      " 53  Origin_MaxInterestRate       8621 non-null   float64\n",
      " 54  Origin_MinInterestRate       8621 non-null   float64\n",
      " 55  Dest_LoanCount               8489 non-null   float64\n",
      " 56  Dest_TotalPrincipal          8489 non-null   float64\n",
      " 57  Dest_AvgInterestRate         8489 non-null   float64\n",
      " 58  Dest_MaxInterestRate         8489 non-null   float64\n",
      " 59  Dest_MinInterestRate         8489 non-null   float64\n",
      " 60  Origin_LoanStatus_AccountID  8621 non-null   float64\n",
      " 61  Origin_LoanStatus_Active     8621 non-null   float64\n",
      " 62  Origin_LoanStatus_Overdue    8621 non-null   float64\n",
      " 63  Origin_LoanStatus_Paid Off   8621 non-null   float64\n",
      " 64  Dest_LoanStatus_AccountID    8489 non-null   float64\n",
      " 65  Dest_LoanStatus_Active       8489 non-null   float64\n",
      " 66  Dest_LoanStatus_Overdue      8489 non-null   float64\n",
      " 67  Dest_LoanStatus_Paid Off     8489 non-null   float64\n",
      "dtypes: float64(33), int64(8), object(27)\n",
      "memory usage: 24.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c02523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting feature engineering process...\n",
      "ğŸ”§ Computing transaction features...\n",
      "  â†’ Amount-based features\n",
      "  â†’ Account status features\n",
      "  â†’ Customer demographic features\n",
      "  â†’ Loan-related features\n",
      "  â†’ Temporal features\n",
      "  â†’ Anomaly detection flags\n",
      "âœ… Feature engineering completed!\n",
      "\n",
      "âœ… Feature engineering completed!\n",
      "Final dataset shape: (48042, 102)\n",
      "Total features: 102\n",
      "\n",
      "ğŸ“ˆ Feature Summary:\n",
      "Original features: 68\n",
      "New features added: 34\n",
      "Total features: 102\n",
      "\n",
      "ğŸ” Sample of new features:\n",
      "New feature count: 34\n",
      "Sample features: ['Amount_to_OriginBalance', 'Amount_to_DestBalance', 'Amount_to_AvgTransaction', 'Origin_AccountInactive', 'Dest_AccountInactive', 'Origin_IsChecking', 'Dest_IsChecking', 'Origin_IsSavings', 'Dest_IsSavings', 'Origin_Age']\n"
     ]
    }
   ],
   "source": [
    "# Execute feature engineering\n",
    "print(\"ğŸš€ Starting feature engineering process...\")\n",
    "\n",
    "df_final_with_features = compute_transaction_features(df_final)\n",
    "\n",
    "print(f\"\\nâœ… Feature engineering completed!\")\n",
    "print(f\"Final dataset shape: {df_final_with_features.shape}\")\n",
    "print(f\"Total features: {df_final_with_features.shape[1]}\")\n",
    "\n",
    "# Display feature summary\n",
    "print(f\"\\nğŸ“ˆ Feature Summary:\")\n",
    "print(f\"Original features: {df_final.shape[1]}\")\n",
    "print(f\"New features added: {df_final_with_features.shape[1] - df_final.shape[1]}\")\n",
    "print(f\"Total features: {df_final_with_features.shape[1]}\")\n",
    "\n",
    "# Show some sample features\n",
    "print(f\"\\nğŸ” Sample of new features:\")\n",
    "new_features = [col for col in df_final_with_features.columns if col not in df_final.columns]\n",
    "print(f\"New feature count: {len(new_features)}\")\n",
    "print(f\"Sample features: {new_features[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc2c88",
   "metadata": {},
   "source": [
    "## Step 4: Data Validation\n",
    "\n",
    "After feature engineering, we validate the dataset to ensure data quality and identify any issues that need to be addressed before modeling.\n",
    "\n",
    "### Validation Checks:\n",
    "\n",
    "1. **Data Completeness**: Check for missing values and data coverage\n",
    "2. **Feature Quality**: Validate engineered features for reasonable ranges\n",
    "3. **Data Consistency**: Ensure logical relationships between features\n",
    "4. **Memory Usage**: Monitor dataset size and memory requirements\n",
    "5. **Feature Distribution**: Check for extreme values and outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7f0c1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Performing data validation...\n",
      "\n",
      "ğŸ“Š Dataset Information:\n",
      "Shape: (48042, 102)\n",
      "Memory usage: 108.19 MB\n",
      "Data types: {dtype('float64'): 41, dtype('int64'): 30, dtype('O'): 27, dtype('int32'): 4}\n",
      "\n",
      "âŒ Missing Values Analysis:\n",
      "Columns with missing values: 59\n",
      "Total missing values: 854449\n",
      "\n",
      "Top 10 columns with missing values:\n",
      "                   Column  Missing_Count  Missing_Percentage\n",
      "     Dest_AvgInterestRate          39553           82.330045\n",
      "     Dest_MaxInterestRate          39553           82.330045\n",
      "     Dest_MinInterestRate          39553           82.330045\n",
      " Dest_LoanStatus_Paid Off          39553           82.330045\n",
      "Dest_LoanStatus_AccountID          39553           82.330045\n",
      "   Dest_LoanStatus_Active          39553           82.330045\n",
      "  Dest_LoanStatus_Overdue          39553           82.330045\n",
      "      Dest_TotalPrincipal          39553           82.330045\n",
      "        Dest_LoanLeverage          39553           82.330045\n",
      "           Dest_LoanCount          39553           82.330045\n",
      "\n",
      "âœ… Feature Quality Checks:\n",
      "âœ… No infinite values found\n",
      "\n",
      "ğŸ” Extreme Values Analysis:\n",
      "Amount: 1st percentile = 48.66, 99th percentile = 4949.50\n",
      "Amount_to_OriginBalance: 1st percentile = 0.00, 99th percentile = 3.17\n",
      "Amount_to_DestBalance: 1st percentile = 0.00, 99th percentile = 3.07\n",
      "Origin_LoanLeverage: 1st percentile = 0.03, 99th percentile = 31.14\n",
      "Dest_LoanLeverage: 1st percentile = 0.03, 99th percentile = 37.02\n",
      "\n",
      "ğŸ“ˆ Feature Ranges:\n",
      "Numeric features: 75\n",
      "\n",
      "ğŸ“‹ Sample Feature Statistics:\n",
      "         Amount  TransactionHour  Origin_Age  Dest_Age  LargeTransferFlag\n",
      "count  48042.00         48042.00    45541.00  45415.00           48042.00\n",
      "mean    2503.42            11.54       44.63     44.63               0.05\n",
      "std     1442.53             6.95       12.19     12.20               0.22\n",
      "min        1.01             0.00       24.00     24.00               0.00\n",
      "25%     1260.41             6.00       34.00     34.00               0.00\n",
      "50%     2503.48            12.00       44.00     44.00               0.00\n",
      "75%     3752.18            18.00       56.00     56.00               0.00\n",
      "max     4999.59            23.00       65.00     65.00               1.00\n",
      "\n",
      "âœ… Data validation completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Data Validation\n",
    "print(\"ğŸ” Performing data validation...\")\n",
    "\n",
    "# Basic dataset information\n",
    "print(f\"\\nğŸ“Š Dataset Information:\")\n",
    "print(f\"Shape: {df_final_with_features.shape}\")\n",
    "print(f\"Memory usage: {df_final_with_features.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Data types: {df_final_with_features.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Missing values analysis\n",
    "print(f\"\\nâŒ Missing Values Analysis:\")\n",
    "missing_values = df_final_with_features.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df_final_with_features)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing_Count': missing_values.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"Columns with missing values: {(missing_values > 0).sum()}\")\n",
    "print(f\"Total missing values: {missing_values.sum()}\")\n",
    "\n",
    "if (missing_values > 0).any():\n",
    "    print(f\"\\nTop 10 columns with missing values:\")\n",
    "    print(missing_df.head(10).to_string(index=False))\n",
    "\n",
    "# Feature quality checks\n",
    "print(f\"\\nâœ… Feature Quality Checks:\")\n",
    "\n",
    "# Check for infinite values\n",
    "infinite_cols = []\n",
    "for col in df_final_with_features.select_dtypes(include=[np.number]).columns:\n",
    "    if np.isinf(df_final_with_features[col]).any():\n",
    "        infinite_cols.append(col)\n",
    "\n",
    "if infinite_cols:\n",
    "    print(f\"âš ï¸  Columns with infinite values: {infinite_cols}\")\n",
    "else:\n",
    "    print(f\"âœ… No infinite values found\")\n",
    "\n",
    "# Check for extreme values in key features\n",
    "print(f\"\\nğŸ” Extreme Values Analysis:\")\n",
    "key_features = ['Amount', 'Amount_to_OriginBalance', 'Amount_to_DestBalance', \n",
    "                'Origin_LoanLeverage', 'Dest_LoanLeverage']\n",
    "\n",
    "for feature in key_features:\n",
    "    if feature in df_final_with_features.columns:\n",
    "        q99 = df_final_with_features[feature].quantile(0.99)\n",
    "        q01 = df_final_with_features[feature].quantile(0.01)\n",
    "        print(f\"{feature}: 1st percentile = {q01:.2f}, 99th percentile = {q99:.2f}\")\n",
    "\n",
    "# Check feature ranges\n",
    "print(f\"\\nğŸ“ˆ Feature Ranges:\")\n",
    "numeric_features = df_final_with_features.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "\n",
    "# Sample of feature statistics\n",
    "print(f\"\\nğŸ“‹ Sample Feature Statistics:\")\n",
    "sample_features = ['Amount', 'TransactionHour', 'Origin_Age', 'Dest_Age', 'LargeTransferFlag']\n",
    "available_features = [f for f in sample_features if f in df_final_with_features.columns]\n",
    "\n",
    "if available_features:\n",
    "    print(df_final_with_features[available_features].describe().round(2))\n",
    "\n",
    "print(f\"\\nâœ… Data validation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca732f",
   "metadata": {},
   "source": [
    "## Step 5: Save Processed Data\n",
    "\n",
    "The final step is to save the enriched and feature-engineered dataset for use in machine learning models.\n",
    "\n",
    "### Output Files:\n",
    "\n",
    "1. **Enriched Transactions**: Complete dataset with all merged information\n",
    "2. **Feature Summary**: Documentation of all engineered features\n",
    "3. **Data Quality Report**: Summary of validation results\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Model Training**: Use this dataset for anomaly detection model training\n",
    "- **Feature Selection**: Identify the most important features for the model\n",
    "- **Model Evaluation**: Test different algorithms and hyperparameters\n",
    "- **Production Deployment**: Integrate into the MLOps pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6c5fe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving processed data...\n",
      "âœ… Dataset saved to: ../data/processed\\enriched_transactions_with_features.csv\n",
      "File size: 26.90 MB\n",
      "\n",
      "ğŸ“‹ Feature Summary:\n",
      "Total_Features: 102\n",
      "Original_Features: 68\n",
      "Engineered_Features: 34\n",
      "Numeric_Features: 75\n",
      "Categorical_Features: 27\n",
      "Boolean_Features: 0\n",
      "Missing_Values: 854449\n",
      "Duplicate_Rows: 0\n",
      "âœ… Feature summary saved to: ../data/processed\\feature_summary.txt\n",
      "\n",
      "ğŸ‰ Feature engineering pipeline completed successfully!\n",
      "ğŸ“Š Final dataset: 48042 rows Ã— 102 columns\n",
      "ğŸ’¾ Data saved to: ../data/processed\\enriched_transactions_with_features.csv\n",
      "ğŸ“‹ Summary saved to: ../data/processed\\feature_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the processed data\n",
    "print(\"ğŸ’¾ Saving processed data...\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "output_dir = \"../data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the enriched dataset\n",
    "output_file = os.path.join(output_dir, \"enriched_transactions_with_features.csv\")\n",
    "df_final_with_features.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"âœ… Dataset saved to: {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Create feature summary\n",
    "feature_summary = {\n",
    "    'Total_Features': len(df_final_with_features.columns),\n",
    "    'Original_Features': len(df_final.columns),\n",
    "    'Engineered_Features': len(df_final_with_features.columns) - df_final.shape[1],\n",
    "    'Numeric_Features': len(df_final_with_features.select_dtypes(include=[np.number]).columns),\n",
    "    'Categorical_Features': len(df_final_with_features.select_dtypes(include=['object']).columns),\n",
    "    'Boolean_Features': len(df_final_with_features.select_dtypes(include=['bool']).columns),\n",
    "    'Missing_Values': df_final_with_features.isnull().sum().sum(),\n",
    "    'Duplicate_Rows': df_final_with_features.duplicated().sum()\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“‹ Feature Summary:\")\n",
    "for key, value in feature_summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Save feature summary\n",
    "feature_summary_file = os.path.join(output_dir, \"feature_summary.txt\")\n",
    "with open(feature_summary_file, 'w') as f:\n",
    "    f.write(\"Feature Engineering Summary\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    for key, value in feature_summary.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nFeature List:\\n\")\n",
    "    f.write(\"-\" * 20 + \"\\n\")\n",
    "    for i, col in enumerate(df_final_with_features.columns, 1):\n",
    "        f.write(f\"{i:3d}. {col}\\n\")\n",
    "\n",
    "print(f\"âœ… Feature summary saved to: {feature_summary_file}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Feature engineering pipeline completed successfully!\")\n",
    "print(f\"ğŸ“Š Final dataset: {df_final_with_features.shape[0]} rows Ã— {df_final_with_features.shape[1]} columns\")\n",
    "print(f\"ğŸ’¾ Data saved to: {output_file}\")\n",
    "print(f\"ğŸ“‹ Summary saved to: {feature_summary_file}\")\n",
    "\n",
    "\n",
    "df_final_with_features.to_csv(\"../data/raw/finalDataSet.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a52c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing basic functionality...\n",
      "âœ… All imports successful!\n",
      "ğŸ“ Testing data loading...\n",
      "âœ… Found 13 CSV files in ../data/processed\n",
      "  - accounts_cleaned.csv\n",
      "  - account_statuses.csv\n",
      "  - account_types.csv\n",
      "âœ… Basic functionality test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test basic functionality\n",
    "print(\"ğŸ§ª Testing basic functionality...\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(\"âœ… All imports successful!\")\n",
    "    \n",
    "    # Test data loading\n",
    "    print(\"ğŸ“ Testing data loading...\")\n",
    "    import os\n",
    "    import glob\n",
    "    \n",
    "    raw_data_path = \"../data/processed\"\n",
    "    if os.path.exists(raw_data_path):\n",
    "        csv_files = glob.glob(os.path.join(raw_data_path, \"*.csv\"))\n",
    "        print(f\"âœ… Found {len(csv_files)} CSV files in {raw_data_path}\")\n",
    "        for csv_file in csv_files[:3]:  # Show first 3 files\n",
    "            print(f\"  - {os.path.basename(csv_file)}\")\n",
    "    else:\n",
    "        print(f\"âŒ Directory {raw_data_path} not found\")\n",
    "    \n",
    "    print(\"âœ… Basic functionality test completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during testing: {str(e)}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc1b3afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Debugging merged dataset columns...\n",
      "Shape of df_final: (48042, 68)\n",
      "\n",
      "Columns in df_final:\n",
      "['TransactionID', 'AccountOriginID', 'AccountDestinationID', 'TransactionTypeID', 'Amount', 'TransactionDate', 'BranchID', 'Description', 'TransactionTypeName', 'Origin_AccountID', 'Origin_CustomerID', 'Origin_AccountTypeID', 'Origin_AccountStatusID', 'Origin_Balance', 'Origin_OpeningDate', 'Origin_AccountType', 'Origin_AccountStatus', 'Dest_AccountID', 'Dest_CustomerID', 'Dest_AccountTypeID', 'Dest_AccountStatusID', 'Dest_Balance', 'Dest_OpeningDate', 'Dest_AccountType', 'Dest_AccountStatus', 'Origin_FirstName', 'Origin_LastName', 'Origin_DateOfBirth', 'Origin_AddressID', 'Origin_CustomerTypeID', 'Origin_TypeName', 'Origin_Street', 'Origin_City', 'Origin_Country', 'Dest_FirstName', 'Dest_LastName', 'Dest_DateOfBirth', 'Dest_AddressID', 'Dest_CustomerTypeID', 'Dest_TypeName', 'Dest_Street', 'Dest_City', 'Dest_Country', 'Branch_BranchID', 'Branch_BranchName', 'Branch_AddressID', 'Branch_Branch_AddressID', 'Branch_Branch_Street', 'Branch_Branch_City', 'Branch_Branch_Country', 'Origin_LoanCount', 'Origin_TotalPrincipal', 'Origin_AvgInterestRate', 'Origin_MaxInterestRate', 'Origin_MinInterestRate', 'Dest_LoanCount', 'Dest_TotalPrincipal', 'Dest_AvgInterestRate', 'Dest_MaxInterestRate', 'Dest_MinInterestRate', 'Origin_LoanStatus_AccountID', 'Origin_LoanStatus_Active', 'Origin_LoanStatus_Overdue', 'Origin_LoanStatus_Paid Off', 'Dest_LoanStatus_AccountID', 'Dest_LoanStatus_Active', 'Dest_LoanStatus_Overdue', 'Dest_LoanStatus_Paid Off']\n",
      "\n",
      "Customer-related columns: ['Origin_CustomerID', 'Dest_CustomerID', 'Origin_CustomerTypeID', 'Dest_CustomerTypeID']\n",
      "\n",
      "Missing customer columns: ['Origin_CustomerType', 'Dest_CustomerType']\n",
      "\n",
      "All columns containing 'Customer':\n",
      "  - Origin_CustomerID\n",
      "  - Dest_CustomerID\n",
      "  - Origin_CustomerTypeID\n",
      "  - Dest_CustomerTypeID\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what columns are available in the merged dataset\n",
    "print(\"ğŸ” Debugging merged dataset columns...\")\n",
    "print(f\"Shape of df_final: {df_final.shape}\")\n",
    "print(f\"\\nColumns in df_final:\")\n",
    "print(df_final.columns.tolist())\n",
    "\n",
    "# Check if customer-related columns exist\n",
    "customer_columns = [col for col in df_final.columns if 'Customer' in col or 'customer' in col]\n",
    "print(f\"\\nCustomer-related columns: {customer_columns}\")\n",
    "\n",
    "# Check if we have the expected customer type columns\n",
    "expected_customer_cols = ['Origin_CustomerType', 'Dest_CustomerType']\n",
    "missing_cols = [col for col in expected_customer_cols if col not in df_final.columns]\n",
    "print(f\"\\nMissing customer columns: {missing_cols}\")\n",
    "\n",
    "# Let's see what customer columns we actually have\n",
    "print(f\"\\nAll columns containing 'Customer':\")\n",
    "for col in df_final.columns:\n",
    "    if 'Customer' in col or 'customer' in col:\n",
    "        print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "320230de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Debugging customer data structure...\n",
      "Customer types columns: ['CustomerTypeID', 'TypeName']\n",
      "Customer types data:\n",
      "   CustomerTypeID          TypeName\n",
      "0               1        Individual\n",
      "1               2    Small Business\n",
      "2               3  Large Enterprise\n",
      "\n",
      "Customers columns: ['CustomerID', 'FirstName', 'LastName', 'DateOfBirth', 'AddressID', 'CustomerTypeID']\n",
      "Customers data sample:\n",
      "   CustomerID FirstName    LastName DateOfBirth  AddressID  CustomerTypeID\n",
      "0       10832      Nyla     Aguirre  1974-02-07        881               1\n",
      "1       10983   Unknown      Battle  1963-02-01        958               2\n",
      "2       10837  Angelena  Harrington  1964-03-25         86               3\n",
      "3       10107    Remona       Glass  1965-09-16        595               1\n",
      "4       10553      King      Becker  1966-02-20        969               3\n",
      "\n",
      "After merging customers with customer types:\n",
      "Columns: ['CustomerID', 'FirstName', 'LastName', 'DateOfBirth', 'AddressID', 'CustomerTypeID', 'TypeName']\n",
      "Sample data:\n",
      "   CustomerID FirstName    LastName DateOfBirth  AddressID  CustomerTypeID  \\\n",
      "0       10832      Nyla     Aguirre  1974-02-07        881               1   \n",
      "1       10983   Unknown      Battle  1963-02-01        958               2   \n",
      "2       10837  Angelena  Harrington  1964-03-25         86               3   \n",
      "3       10107    Remona       Glass  1965-09-16        595               1   \n",
      "4       10553      King      Becker  1966-02-20        969               3   \n",
      "\n",
      "           TypeName  \n",
      "0        Individual  \n",
      "1    Small Business  \n",
      "2  Large Enterprise  \n",
      "3        Individual  \n",
      "4  Large Enterprise  \n"
     ]
    }
   ],
   "source": [
    "# Debug: Check customer data structure\n",
    "print(\"ğŸ” Debugging customer data structure...\")\n",
    "print(f\"Customer types columns: {df_customer_types.columns.tolist()}\")\n",
    "print(f\"Customer types data:\")\n",
    "print(df_customer_types.head())\n",
    "\n",
    "print(f\"\\nCustomers columns: {df_customers.columns.tolist()}\")\n",
    "print(f\"Customers data sample:\")\n",
    "print(df_customers.head())\n",
    "\n",
    "# Check what happens when we merge customers with customer types\n",
    "customers_full = df_customers.merge(df_customer_types, on=\"CustomerTypeID\", how=\"left\")\n",
    "print(f\"\\nAfter merging customers with customer types:\")\n",
    "print(f\"Columns: {customers_full.columns.tolist()}\")\n",
    "print(f\"Sample data:\")\n",
    "print(customers_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c01a9ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionID', 'AccountOriginID', 'AccountDestinationID',\n",
       "       'TransactionTypeID', 'Amount', 'TransactionDate', 'BranchID',\n",
       "       'Description', 'TransactionTypeName', 'Origin_AccountID',\n",
       "       ...\n",
       "       'TransactionMonth', 'TransactionQuarter', 'IsWeekend',\n",
       "       'IsBusinessHours', 'IsNightTime', 'LargeTransferFlag',\n",
       "       'VeryLargeTransferFlag', 'UnusualTimingFlag', 'HighRiskFlag',\n",
       "       'CrossTypeTransfer'],\n",
       "      dtype='object', length=102)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_with_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c608a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=df_final_with_features\n",
    "\n",
    "# List of necessary features for anomaly detection\n",
    "features_to_keep = [\n",
    "    \"TransactionTypeID\", \"Amount\",\n",
    "    \"Origin_AccountTypeID\", \"Origin_AccountStatusID\", \"Origin_Balance\",\n",
    "    \"Dest_AccountTypeID\", \"Dest_AccountStatusID\", \"Dest_Balance\",\n",
    "    \"Origin_CustomerTypeID\", \"Dest_CustomerTypeID\",\n",
    "    \"Origin_LoanCount\", \"Origin_TotalPrincipal\", \"Origin_AvgInterestRate\",\n",
    "    \"Dest_LoanCount\", \"Dest_TotalPrincipal\", \"Dest_AvgInterestRate\",\n",
    "    \"Origin_LoanStatus_Active\", \"Origin_LoanStatus_Overdue\", \"Origin_LoanStatus_Paid Off\",\n",
    "    \"Dest_LoanStatus_Active\", \"Dest_LoanStatus_Overdue\", \"Dest_LoanStatus_Paid Off\",\n",
    "    \"Amount_to_OriginBalance\", \"Amount_to_DestBalance\", \"Amount_to_AvgTransaction\",\n",
    "    \"Origin_AccountInactive\", \"Dest_AccountInactive\", \"Age_Difference\",\n",
    "    \"Origin_LoanLeverage\", \"Dest_LoanLeverage\",\n",
    "    \"TransactionHour\", \"TransactionWeekday\", \"TransactionMonth\", \"TransactionQuarter\",\n",
    "    \"IsWeekend\", \"IsBusinessHours\", \"IsNightTime\",\n",
    "    \"LargeTransferFlag\", \"VeryLargeTransferFlag\", \"UnusualTimingFlag\", \"HighRiskFlag\", \"CrossTypeTransfer\"\n",
    "]\n",
    "\n",
    "# Keep only necessary features\n",
    "df_important_features = df[features_to_keep]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00462e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionTypeID</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Origin_AccountTypeID</th>\n",
       "      <th>Origin_AccountStatusID</th>\n",
       "      <th>Origin_Balance</th>\n",
       "      <th>Dest_AccountTypeID</th>\n",
       "      <th>Dest_AccountStatusID</th>\n",
       "      <th>Dest_Balance</th>\n",
       "      <th>Origin_CustomerTypeID</th>\n",
       "      <th>Dest_CustomerTypeID</th>\n",
       "      <th>Origin_LoanCount</th>\n",
       "      <th>Origin_TotalPrincipal</th>\n",
       "      <th>Origin_AvgInterestRate</th>\n",
       "      <th>Dest_LoanCount</th>\n",
       "      <th>Dest_TotalPrincipal</th>\n",
       "      <th>Dest_AvgInterestRate</th>\n",
       "      <th>Origin_LoanStatus_Active</th>\n",
       "      <th>Origin_LoanStatus_Overdue</th>\n",
       "      <th>Origin_LoanStatus_Paid Off</th>\n",
       "      <th>Dest_LoanStatus_Active</th>\n",
       "      <th>Dest_LoanStatus_Overdue</th>\n",
       "      <th>Dest_LoanStatus_Paid Off</th>\n",
       "      <th>Amount_to_OriginBalance</th>\n",
       "      <th>Amount_to_DestBalance</th>\n",
       "      <th>Amount_to_AvgTransaction</th>\n",
       "      <th>Origin_AccountInactive</th>\n",
       "      <th>Dest_AccountInactive</th>\n",
       "      <th>Age_Difference</th>\n",
       "      <th>Origin_LoanLeverage</th>\n",
       "      <th>Dest_LoanLeverage</th>\n",
       "      <th>TransactionHour</th>\n",
       "      <th>TransactionWeekday</th>\n",
       "      <th>TransactionMonth</th>\n",
       "      <th>TransactionQuarter</th>\n",
       "      <th>IsWeekend</th>\n",
       "      <th>IsBusinessHours</th>\n",
       "      <th>IsNightTime</th>\n",
       "      <th>LargeTransferFlag</th>\n",
       "      <th>VeryLargeTransferFlag</th>\n",
       "      <th>UnusualTimingFlag</th>\n",
       "      <th>HighRiskFlag</th>\n",
       "      <th>CrossTypeTransfer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2984.02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55292.55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>54095.48</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52255.85</td>\n",
       "      <td>0.12830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.053968</td>\n",
       "      <td>0.055162</td>\n",
       "      <td>1.191976</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.945079</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4713.48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22940.53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70727.24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>146020.58</td>\n",
       "      <td>0.07795</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59673.83</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205465</td>\n",
       "      <td>0.066643</td>\n",
       "      <td>1.882814</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.365179</td>\n",
       "      <td>0.843718</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1600.23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62435.08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>90061.73</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22288.38</td>\n",
       "      <td>0.09690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025630</td>\n",
       "      <td>0.017768</td>\n",
       "      <td>0.639217</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.356985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4279.61</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12739.44</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32623.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15797.11</td>\n",
       "      <td>0.06360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97362.97</td>\n",
       "      <td>0.1145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.335934</td>\n",
       "      <td>0.131181</td>\n",
       "      <td>1.709504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.240016</td>\n",
       "      <td>2.984420</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4125.32</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56605.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37284.69</td>\n",
       "      <td>0.07180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.647872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.658676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionTypeID   Amount  Origin_AccountTypeID  Origin_AccountStatusID  \\\n",
       "0                  3  2984.02                   5.0                     1.0   \n",
       "1                  3  4713.48                   1.0                     1.0   \n",
       "2                  3  1600.23                   4.0                     1.0   \n",
       "3                  1  4279.61                   4.0                     1.0   \n",
       "4                  2  4125.32                   4.0                     1.0   \n",
       "\n",
       "   Origin_Balance  Dest_AccountTypeID  Dest_AccountStatusID  Dest_Balance  \\\n",
       "0        55292.55                 2.0                   2.0      54095.48   \n",
       "1        22940.53                 4.0                   1.0      70727.24   \n",
       "2        62435.08                 5.0                   2.0      90061.73   \n",
       "3        12739.44                 3.0                   1.0      32623.75   \n",
       "4        56605.54                 NaN                   NaN           NaN   \n",
       "\n",
       "   Origin_CustomerTypeID  Dest_CustomerTypeID  Origin_LoanCount  \\\n",
       "0                    2.0                  1.0               1.0   \n",
       "1                    3.0                  3.0               2.0   \n",
       "2                    2.0                  1.0               1.0   \n",
       "3                    3.0                  3.0               1.0   \n",
       "4                    2.0                  NaN               1.0   \n",
       "\n",
       "   Origin_TotalPrincipal  Origin_AvgInterestRate  Dest_LoanCount  \\\n",
       "0               52255.85                 0.12830             NaN   \n",
       "1              146020.58                 0.07795             1.0   \n",
       "2               22288.38                 0.09690             NaN   \n",
       "3               15797.11                 0.06360             1.0   \n",
       "4               37284.69                 0.07180             NaN   \n",
       "\n",
       "   Dest_TotalPrincipal  Dest_AvgInterestRate  Origin_LoanStatus_Active  \\\n",
       "0                  NaN                   NaN                       0.0   \n",
       "1             59673.83                0.1411                       2.0   \n",
       "2                  NaN                   NaN                       1.0   \n",
       "3             97362.97                0.1145                       1.0   \n",
       "4                  NaN                   NaN                       1.0   \n",
       "\n",
       "   Origin_LoanStatus_Overdue  Origin_LoanStatus_Paid Off  \\\n",
       "0                        1.0                         0.0   \n",
       "1                        0.0                         0.0   \n",
       "2                        0.0                         0.0   \n",
       "3                        0.0                         0.0   \n",
       "4                        0.0                         0.0   \n",
       "\n",
       "   Dest_LoanStatus_Active  Dest_LoanStatus_Overdue  Dest_LoanStatus_Paid Off  \\\n",
       "0                     NaN                      NaN                       NaN   \n",
       "1                     1.0                      0.0                       0.0   \n",
       "2                     NaN                      NaN                       NaN   \n",
       "3                     1.0                      0.0                       0.0   \n",
       "4                     NaN                      NaN                       NaN   \n",
       "\n",
       "   Amount_to_OriginBalance  Amount_to_DestBalance  Amount_to_AvgTransaction  \\\n",
       "0                 0.053968               0.055162                  1.191976   \n",
       "1                 0.205465               0.066643                  1.882814   \n",
       "2                 0.025630               0.017768                  0.639217   \n",
       "3                 0.335934               0.131181                  1.709504   \n",
       "4                 0.072878                    NaN                  1.647872   \n",
       "\n",
       "   Origin_AccountInactive  Dest_AccountInactive  Age_Difference  \\\n",
       "0                       0                     1             4.0   \n",
       "1                       0                     0            16.0   \n",
       "2                       0                     1             6.0   \n",
       "3                       0                     0            -3.0   \n",
       "4                       0                     0             NaN   \n",
       "\n",
       "   Origin_LoanLeverage  Dest_LoanLeverage  TransactionHour  \\\n",
       "0             0.945079                NaN               10   \n",
       "1             6.365179           0.843718                2   \n",
       "2             0.356985                NaN               17   \n",
       "3             1.240016           2.984420                7   \n",
       "4             0.658676                NaN                2   \n",
       "\n",
       "   TransactionWeekday  TransactionMonth  TransactionQuarter  IsWeekend  \\\n",
       "0                   4                12                   4          0   \n",
       "1                   4                 5                   2          0   \n",
       "2                   1                 7                   3          0   \n",
       "3                   5                 9                   3          1   \n",
       "4                   6                 5                   2          1   \n",
       "\n",
       "   IsBusinessHours  IsNightTime  LargeTransferFlag  VeryLargeTransferFlag  \\\n",
       "0                1            0                  0                      0   \n",
       "1                0            1                  0                      0   \n",
       "2                1            0                  0                      0   \n",
       "3                0            0                  0                      0   \n",
       "4                0            1                  0                      0   \n",
       "\n",
       "   UnusualTimingFlag  HighRiskFlag  CrossTypeTransfer  \n",
       "0                  0             1                  1  \n",
       "1                  1             1                  0  \n",
       "2                  0             1                  1  \n",
       "3                  1             1                  0  \n",
       "4                  1             0                  1  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_important_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ef39e978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining NaNs: 566670\n"
     ]
    }
   ],
   "source": [
    "print(\"Remaining NaNs:\", df_important_features.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ad82c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining NaNs: 566670\n"
     ]
    }
   ],
   "source": [
    "df_important_features.fillna(0)\n",
    "print(\"Remaining NaNs:\", df_important_features.isna().sum().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
